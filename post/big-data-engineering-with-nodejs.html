<!DOCTYPE html><html><head><meta name="viewport" content="width=device-width"/><meta charSet="utf-8"/><title>Jason Kim&#x27;s Blog</title><link rel="icon" href="/favicon.ico"/><meta name="next-head-count" content="4"/><link rel="preload" href="/_next/static/css/5398d8c2c6eb5e22e315.css" as="style"/><link rel="stylesheet" href="/_next/static/css/5398d8c2c6eb5e22e315.css"/><link rel="preload" href="/_next/static/css/3c6bf924f0424cd47cc5.css" as="style"/><link rel="stylesheet" href="/_next/static/css/3c6bf924f0424cd47cc5.css"/><link rel="preload" href="/_next/static/0KCVTKrcnz1vaknDlOdeE/pages/_app.js" as="script"/><link rel="preload" href="/_next/static/0KCVTKrcnz1vaknDlOdeE/pages/post/%5Bslug%5D.js" as="script"/><link rel="preload" href="/_next/static/runtime/webpack-c212667a5f965e81e004.js" as="script"/><link rel="preload" href="/_next/static/chunks/framework.e84fa698c7ee940652bd.js" as="script"/><link rel="preload" href="/_next/static/chunks/5a4765bf.4c419f4b16a96bca7647.js" as="script"/><link rel="preload" href="/_next/static/chunks/commons.212dae668aecc379ad96.js" as="script"/><link rel="preload" href="/_next/static/runtime/main-c20c68f2e453b0454f3a.js" as="script"/><link rel="preload" href="/_next/static/chunks/75fc9c18.cbe862d05f67479ee6e5.js" as="script"/><link rel="preload" href="/_next/static/chunks/682f061b037abbb67de245157a69db824df2f8a5.64565d5a878629f53174.js" as="script"/></head><body><div id="__next"><div class="container mx-auto"><header class="pt-16 pb-12"><a class="hover:underline" href="/"><h1>Jason Kim&#x27;s Blog</h1></a></header><main class="main pb-8"><div class="pb-2 text-3xl font-bold">&quot;Big Data&quot; Engineering with Node.js</div><div class="pb-8 text-sm text-gray-600">2019-04-26</div><div class="markdown"><p>When you think about data engineering, people usually think about
using Java or Python as the primary languages to work with.
The two languages both have a large community backing and many tools
for handling big data.
While they are fine languages to work with, I want to throw Node.js
into the mix as a serious contender to get the job done.
Node.js is performant enough to do some heavy lifting and has great NPM packages
that help you develop fast. I wanted to share my experience of building data-intensive systems using Node.js.</p>
<p>I've been working at <a href="https://www.coupang.com/">Coupang</a> for just over a year now. I had the privilege of building the first
<a href="https://en.wikipedia.org/wiki/Extract,_transform,_load">ETL</a>
system for Coupang's advertising platform.
The ETL system began with a modest goal to clean and ingest advertising
metadata and time series data on a daily basis.
Since the original ETL jobs began every day in the morning, I named
the ETL system, <code>Morning Star</code>, referring to planet Venus that appears in
the east before sunrise.
This is a bit of a misnomer now, given that the system is now running
ETL jobs on a real-time basis. I still kept the name.
Since its inception, Morning Star evolved into a much larger system
that is handling 500 GB of data, 400 million events per day, and the data volume is still growing.</p>
<p>Morning Star needed to provide data to several kinds of users. Services
needed to retrieve appropriate data reliably in a timely fashion.
Analysts asked for a place where they can gather insights about our business. Also, Morning Star needed to provide reliable, normalized data for creating denormalized tables for specific use cases. </p>
<p>Let's examine the basic data flow of Morning Star. Morning Star's
infrastructure lives on AWS.
The data flow starts with several Lambda functions that trigger
different jobs for Morning Star by making an HTTP request to the
load balancer.
The load balancer balances the load in a round robin fashion to
EC2 instances. Based on the CPU usages, the EC2 instances dynamically
scale up or down to efficiently handle the load.
The EC2 instance that is given the request to handle a job, downloads
the raw files from S3 Data Lake. The raw files in S3 originate
from several other
services. Morning Star cleans, enriches and anonymizes the raw data depending on which job it has to perform.
The massaged data is then uploaded back to the data lake, where
it is subsequently copied to Redshift.
Redshift is where various other services and users can consume the
data to their liking.</p>
<p><img src="https://i.imgur.com/BsZ1i9t.png"></p>
<p>Node.js has been an excellent tool for building the desired system.
Node.js finally has the <code>async-await</code> pattern to take advantage of
parallelism without callbacks.
Having worked with callbacks and chained promises, I think
the async-await pattern solved major problems found in the previous
asynchronous programming patterns.
You can program your code spanning vertically rather than horizontally
like the nested callbacks make you do. Your code is easier to read
when it spans vertically.
You can handle errors more precisely than the chained promises pattern where
a single catch block is typically responsible for errors
thrown anywhere during the execution of multiple chained
promises.
You can have a more nimble control flow of your code with <code>async-await</code>
because you can use <code>if-else</code> statements easier.
The ergonomics of coding was much improved with using async-await.</p>
<p>Speaking of great ergonomics, I should also point out the <code>stream</code>
module in Node.js to be a fantastic tool to process large files.
When you work on web services, you typically read and write
discrete chunks of data transactionally.
On the other hand, it is very common to use stream to move
data from one place another in data engineering.
You often deal with cases where you no longer can load
the entire data onto the memory.
Or you might deal with a source that is continuously generating data that
needs to be moved to another place in real time.
That's when you have to stream the data.
Node's stream library helps you stream data reliably and manipulate
the data with ease.
The functions in the stream module are easy to understand and use.
They are well documented with an ample amount of examples, so you can easily
incorporate them into your code.
The stream library plays nicely with the async-await pattern I use
elsewhere as long as I wrap the stream code with a promise.</p>
<p>There are other standard modules in Node.js that makes it
suitable to do data engineering.
The <code>child-process</code> module is used to execute terminal commands,
and it was useful in controlling
<a href="https://stedolan.github.io/jq/">jq</a>,
<a href="https://www.gnu.org/software/parallel/">parallel</a> and
<a href="https://cwiki.apache.org/confluence/display/Hive/HiveServer2+Clients#HiveServer2Clients-Beeline%E2%80%93CommandLineShell">Hive Beeline client</a>.
The <code>fs</code> module had all the functions I needed to work with the file system,
and <code>zlib</code> handle gzipping and gunzipping with stream perfectly.
And if I didn't find an adequate module in the standard Node.js modules,
I found almost all of them in the NPM repository.
This assurance gave me the confidence to push development in Node.js,
and help me iterate faster.</p>
<p>Node.js has some downsides when you try to use it to process data.
I applauded Node.js' async-await to be an excellent pattern to use.
However, this means that you have to write a promise wrapper for
callbacks and pipe functions in order to incorporate it with the
async-await pattern. It's not difficult, but it is tedious.</p>
<p>I also had trouble finding an up to date Hive client for Node.js.
Hive is a widely used data warehousing tool for working with big data.
I was surprised to learn that there's no well maintained
Hive client available to use on NPM.
For this reason, I had to use Hive's Beeline command line client and
write Node wrapper around it.
I had to install JDK, Hadoop, and Hive as dependencies for Beeline
to run.
There were a lot of unpleasant programming I had to do to make this
work.</p>
<p>Despite some of the problems with Node.js, it is still a great tool
for data engineering. It has many standard and third-party modules
to help to finish work. Node.js is also fast enough to process large datasets
in real-time.
I knew that Node.js is great at creating nice web apps and services.
I am glad to know that it is also a fine tool for data engineering.</p>
</div></main><section style="width:100%"></section><footer class="pb-12"><div class="flex"><a class="flex-initial pr-4 hover:underline" href="http://jasonkim.ca/">Personal Site</a><a class="flex-initial pr-4 hover:underline" href="https://github.com/serv">Github</a><a class="flex-initial pr-4 hover:underline" href="https://www.linkedin.com/in/jason-kim-a2b97b6/">Linkedin</a><a class="flex-initial pr-4 hover:underline" href="https://twitter.com/jasoki">Twitter</a></div></footer></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"post":{"title":"\"Big Data\" Engineering with Node.js","createdAt":"2019-04-26T00:00:00-07:00","categories":["node","javascript"],"slug":"big-data-engineering-with-nodejs","fullPath":"/Users/jasonkim/projects/websites/serv.github.io/_posts/2019-04-20-data-engineering-with-nodejs.md","content":"\u003cp\u003eWhen you think about data engineering, people usually think about\nusing Java or Python as the primary languages to work with.\nThe two languages both have a large community backing and many tools\nfor handling big data.\nWhile they are fine languages to work with, I want to throw Node.js\ninto the mix as a serious contender to get the job done.\nNode.js is performant enough to do some heavy lifting and has great NPM packages\nthat help you develop fast. I wanted to share my experience of building data-intensive systems using Node.js.\u003c/p\u003e\n\u003cp\u003eI've been working at \u003ca href=\"https://www.coupang.com/\"\u003eCoupang\u003c/a\u003e for just over a year now. I had the privilege of building the first\n\u003ca href=\"https://en.wikipedia.org/wiki/Extract,_transform,_load\"\u003eETL\u003c/a\u003e\nsystem for Coupang's advertising platform.\nThe ETL system began with a modest goal to clean and ingest advertising\nmetadata and time series data on a daily basis.\nSince the original ETL jobs began every day in the morning, I named\nthe ETL system, \u003ccode\u003eMorning Star\u003c/code\u003e, referring to planet Venus that appears in\nthe east before sunrise.\nThis is a bit of a misnomer now, given that the system is now running\nETL jobs on a real-time basis. I still kept the name.\nSince its inception, Morning Star evolved into a much larger system\nthat is handling 500 GB of data, 400 million events per day, and the data volume is still growing.\u003c/p\u003e\n\u003cp\u003eMorning Star needed to provide data to several kinds of users. Services\nneeded to retrieve appropriate data reliably in a timely fashion.\nAnalysts asked for a place where they can gather insights about our business. Also, Morning Star needed to provide reliable, normalized data for creating denormalized tables for specific use cases. \u003c/p\u003e\n\u003cp\u003eLet's examine the basic data flow of Morning Star. Morning Star's\ninfrastructure lives on AWS.\nThe data flow starts with several Lambda functions that trigger\ndifferent jobs for Morning Star by making an HTTP request to the\nload balancer.\nThe load balancer balances the load in a round robin fashion to\nEC2 instances. Based on the CPU usages, the EC2 instances dynamically\nscale up or down to efficiently handle the load.\nThe EC2 instance that is given the request to handle a job, downloads\nthe raw files from S3 Data Lake. The raw files in S3 originate\nfrom several other\nservices. Morning Star cleans, enriches and anonymizes the raw data depending on which job it has to perform.\nThe massaged data is then uploaded back to the data lake, where\nit is subsequently copied to Redshift.\nRedshift is where various other services and users can consume the\ndata to their liking.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://i.imgur.com/BsZ1i9t.png\"\u003e\u003c/p\u003e\n\u003cp\u003eNode.js has been an excellent tool for building the desired system.\nNode.js finally has the \u003ccode\u003easync-await\u003c/code\u003e pattern to take advantage of\nparallelism without callbacks.\nHaving worked with callbacks and chained promises, I think\nthe async-await pattern solved major problems found in the previous\nasynchronous programming patterns.\nYou can program your code spanning vertically rather than horizontally\nlike the nested callbacks make you do. Your code is easier to read\nwhen it spans vertically.\nYou can handle errors more precisely than the chained promises pattern where\na single catch block is typically responsible for errors\nthrown anywhere during the execution of multiple chained\npromises.\nYou can have a more nimble control flow of your code with \u003ccode\u003easync-await\u003c/code\u003e\nbecause you can use \u003ccode\u003eif-else\u003c/code\u003e statements easier.\nThe ergonomics of coding was much improved with using async-await.\u003c/p\u003e\n\u003cp\u003eSpeaking of great ergonomics, I should also point out the \u003ccode\u003estream\u003c/code\u003e\nmodule in Node.js to be a fantastic tool to process large files.\nWhen you work on web services, you typically read and write\ndiscrete chunks of data transactionally.\nOn the other hand, it is very common to use stream to move\ndata from one place another in data engineering.\nYou often deal with cases where you no longer can load\nthe entire data onto the memory.\nOr you might deal with a source that is continuously generating data that\nneeds to be moved to another place in real time.\nThat's when you have to stream the data.\nNode's stream library helps you stream data reliably and manipulate\nthe data with ease.\nThe functions in the stream module are easy to understand and use.\nThey are well documented with an ample amount of examples, so you can easily\nincorporate them into your code.\nThe stream library plays nicely with the async-await pattern I use\nelsewhere as long as I wrap the stream code with a promise.\u003c/p\u003e\n\u003cp\u003eThere are other standard modules in Node.js that makes it\nsuitable to do data engineering.\nThe \u003ccode\u003echild-process\u003c/code\u003e module is used to execute terminal commands,\nand it was useful in controlling\n\u003ca href=\"https://stedolan.github.io/jq/\"\u003ejq\u003c/a\u003e,\n\u003ca href=\"https://www.gnu.org/software/parallel/\"\u003eparallel\u003c/a\u003e and\n\u003ca href=\"https://cwiki.apache.org/confluence/display/Hive/HiveServer2+Clients#HiveServer2Clients-Beeline%E2%80%93CommandLineShell\"\u003eHive Beeline client\u003c/a\u003e.\nThe \u003ccode\u003efs\u003c/code\u003e module had all the functions I needed to work with the file system,\nand \u003ccode\u003ezlib\u003c/code\u003e handle gzipping and gunzipping with stream perfectly.\nAnd if I didn't find an adequate module in the standard Node.js modules,\nI found almost all of them in the NPM repository.\nThis assurance gave me the confidence to push development in Node.js,\nand help me iterate faster.\u003c/p\u003e\n\u003cp\u003eNode.js has some downsides when you try to use it to process data.\nI applauded Node.js' async-await to be an excellent pattern to use.\nHowever, this means that you have to write a promise wrapper for\ncallbacks and pipe functions in order to incorporate it with the\nasync-await pattern. It's not difficult, but it is tedious.\u003c/p\u003e\n\u003cp\u003eI also had trouble finding an up to date Hive client for Node.js.\nHive is a widely used data warehousing tool for working with big data.\nI was surprised to learn that there's no well maintained\nHive client available to use on NPM.\nFor this reason, I had to use Hive's Beeline command line client and\nwrite Node wrapper around it.\nI had to install JDK, Hadoop, and Hive as dependencies for Beeline\nto run.\nThere were a lot of unpleasant programming I had to do to make this\nwork.\u003c/p\u003e\n\u003cp\u003eDespite some of the problems with Node.js, it is still a great tool\nfor data engineering. It has many standard and third-party modules\nto help to finish work. Node.js is also fast enough to process large datasets\nin real-time.\nI knew that Node.js is great at creating nice web apps and services.\nI am glad to know that it is also a fine tool for data engineering.\u003c/p\u003e\n"}},"__N_SSG":true},"page":"/post/[slug]","query":{"slug":"big-data-engineering-with-nodejs"},"buildId":"0KCVTKrcnz1vaknDlOdeE","nextExport":false,"isFallback":false,"gsp":true}</script><script nomodule="" src="/_next/static/runtime/polyfills-788a592a52ba46c11fcc.js"></script><script async="" data-next-page="/_app" src="/_next/static/0KCVTKrcnz1vaknDlOdeE/pages/_app.js"></script><script async="" data-next-page="/post/[slug]" src="/_next/static/0KCVTKrcnz1vaknDlOdeE/pages/post/%5Bslug%5D.js"></script><script src="/_next/static/runtime/webpack-c212667a5f965e81e004.js" async=""></script><script src="/_next/static/chunks/framework.e84fa698c7ee940652bd.js" async=""></script><script src="/_next/static/chunks/5a4765bf.4c419f4b16a96bca7647.js" async=""></script><script src="/_next/static/chunks/commons.212dae668aecc379ad96.js" async=""></script><script src="/_next/static/runtime/main-c20c68f2e453b0454f3a.js" async=""></script><script src="/_next/static/chunks/75fc9c18.cbe862d05f67479ee6e5.js" async=""></script><script src="/_next/static/chunks/682f061b037abbb67de245157a69db824df2f8a5.64565d5a878629f53174.js" async=""></script><script src="/_next/static/0KCVTKrcnz1vaknDlOdeE/_buildManifest.js" async=""></script><script src="/_next/static/0KCVTKrcnz1vaknDlOdeE/_ssgManifest.js" async=""></script></body></html>