{"pageProps":{"post":{"title":"Fixing Memory Leak on Production Node.js Application","createdAt":"2016-06-02T00:00:00-07:00","categories":["node"],"slug":"fixing-memory-leak-on-production-nodejs-application","fullPath":"/Users/jasonkim/projects/websites/serv.github.io/_posts/2016-06-02-fixing-memory-leak-on-production-node-application.md","content":"<p>The last few days at work were rough.\nMy team was intensely focused on\npreparing the production environment to be stable.\nWe've been having some\nserious issues on the production environment.\nOne of the most serious production issues was a\nnasty memory leak.</p>\n<p><img src=\"http://i.imgur.com/lEnDrWZ.png\"></p>\n<p>The graph above displays the memory usage of\n12 node applications on our production\nenvironment. The y-axis shows the memory usage\nin percentage and the x-axis shows the time\nspan of 7 days. This narrow sawtooth\npattern on memory usage is extremely bad.\nAs you can see, all 12 servers\nare accumulating usage in memory rapidly.\nAs a remedy for the memory leak, we had to\nperiodically restart our servers. This was less\nthan ideal, but because we had so many other\nreally high priority items last few weeks, we\njust had to bite our tongue and suck it up.\nHowever, the frequency at which we had to\nrestart our server started to increase.\nWe had to restart our servers every few days,\nthen every day, then it came to a point where we\nwere restarting our servers every 4 to 5 hours.\nThank god we have a globally distributed team\n(Croatia, and Argentina), it could've been a\nlot tougher without having team members in other\ntimezones. I can't\nemphasize this point enough, and I will take\nanother opportunity to praise having globally\ndistributed software development team in another\npost in the future.</p>\n<p>After some <a href=\"http://blog.jasonkim.ca/blog/2016/06/02/battle-technical-assumptions/\">pitfalls</a>, we fixed the\nmemory leak. As you can see below, the overall\nmemory usage stays flat after 13:30 PM after our\nmemory leak fix was applied.</p>\n<p><img src=\"http://i.imgur.com/JnCgcoi.png\"></p>\n<p>We came up with several different strategies to\nfix the memory issue, but the working solution\ncame down to comparing two memory dumps at\ndifferent times and comparing their content.\nI used <a href=\"http://man7.org/linux/man-pages/man1/gcore.1.html\">gcore</a> on a production server to gather a\nmemory dump soon after the server restart when\nthe memory usage is around 30%. After around\n3 hours when the memory usage for the server\nstarts to hover around 60%, I took another memory\ndump. I used <a href=\"https://docs.oracle.com/cd/E18752_01/html/816-5041/chapter-8.html\">mdb</a> on a local VM running\nSolaris 11 to analyze the two memory dumps.</p>\n<p>Here is the memory dump of the production server\nat around 30% memory usage.</p>\n<p>{% highlight text %}\n73ce2207ea9 4643 2 Arguments: 0, length\n101b5ab4f71 5519 4 Object: albumId, albumNames, ...\n9719767799 6070 2 Object: localeCode, value\n971977c371 6966 1 Object: entry\n1102bfc1dc31 8775 2 Object: id, genreIds\n971977b021 9086 2 Object: ids, href\n971973e051 9430 3 Array\n3015e01 16203 0 Object\n971974c9c9 19854 1 Object: nr_exclusive_duration_millis\n9719710199 19917 13 TraceSegment: name, transaction, ...\n971971cb11 20186 7 Timer: state, touched, duration, ...\n342d8d9 49769 0 Array\n971971cbb9 59489 1 Array\n971970f1c9 71743 2 Array\nOBJECT #OBJECTS #PROPS CONSTRUCTOR: PROPS\n{% endhighlight %}</p>\n<p>And here is the memory dump of the production\nserver at around 80% around 4 hours after\nthe server restart.</p>\n<p>{% highlight text %}\n1102bfc1dc31 8775 2 Object: id, genreIds</p>\n<p>...</p>\n<p>110879a2c9 14099 6 ExponentialBackoffStrategy: ...\n1108779561 15593 3 Array\n45357fdf31 17059 2 Object: ids, href\n3015db1 34680 0 Object\n110875b221 35287 4 Object: albumId, albumNames, ...\n110875b2c9 38401 1 Object: entry\n110875aff1 38481 2 Object: localeCode, value\n1108708d51 198161 0 Array\n1108706ae1 202984 1 Array\n1108706a81 206039 1 Object: nr_exclusive_duration_millis\n11087046d9 209896 13 TraceSegment: name, transaction, ...\n1108704801 223481 7 Timer: state, touched, duration, ...\n1108706aa1 507224 2 Array\nOBJECT #OBJECTS #PROPS CONSTRUCTOR: PROPS\n{% endhighlight %}</p>\n<p>The most notable cause of the memory increase is\n<code>TraceSegment: name, transaction, ...</code>.\nYou can see that the number of <code>TraceSegment</code>\nobject count increased by 10 folds going\nfrom 19917 to 209896. You can also see some\nother object such as <code>Timer</code> and <code>Array</code>\nincreasing. This is because those are properties\nof the <code>TraceSegment</code> object.\nContrast this number with the genre-tags object\nthat stayed exactly the same\n<code>2 Object: id, genreIds</code> at 8775. That object\nis a json file parsed into memory at startup\nof the app and it does not increase. This\nsuggests to me that the memory dump is indeed\naccurate, and Node retains memory steady on\ncorrectly coded objects.</p>\n<p>It turns out <code>TraceSegment</code> objects are created\nby <a href=\"https://github.com/newrelic/node-newrelic/blob/1e7bbbaf34a15f0bb35aca63b0d8a3cfa2669d27/lib/transaction/trace/segment.js\">NewRelic</a>. The fact that\nI wasn't able to recreate the severity of\nmemory leak convincingly on the development\nenvironment from previous attempts also tells\nme that the memory leak was caused by something\nhappening on the production environment.</p>\n<p>We get some benefits of having NewRelic monitoring, but disabling NewRelic on\nproduction was no-brainer at this point.\nWe had to do it. We all acknowledged that\nNewRelic is not the initial cause of the memory leak.\nThere is still something in our code that is\nbehaving naughtily with NewRelic, but we\ngot to put our the damn fire on production\nfirst. So we did it. We disabled NewRelic\non production and the memory usage stopped\nclimbing and it remains very steady for now.</p>\n<p>I want to thank <a href=\"https://yunong.io/2015/11/13/debugging-node-js-in-production/\">Yunong Xiao for sharing\nNetflix's experience on debugging Node.js in\nproduction</a>. It informed me about\nuseful tools in the wild and taught me how\nI can use those tools to find the cause\nof the memory leak. I also want to thank\n<a href=\"http://bryce.is/writing/code/mdb_v8/vm/virtualbox/omnios/2016/02/06/mdbv8-getting-started.html\">Bryce Neal for writing a detailed tutorial on setting up Solaris to use MDB</a>.</p>\n"}},"__N_SSG":true}