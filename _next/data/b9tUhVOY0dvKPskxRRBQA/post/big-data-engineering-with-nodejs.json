{"pageProps":{"post":{"title":"\"Big Data\" Engineering with Node.js","createdAt":"2019-04-26T00:00:00-07:00","categories":["node","javascript"],"slug":"big-data-engineering-with-nodejs","fullPath":"/home/jason/projects/websites/serv.github.io/_posts/2019-04-20-data-engineering-with-nodejs.md","content":"<p>When you think about data engineering, people usually think about\nusing Java or Python as the primary languages to work with.\nThe two languages both have a large community backing and many tools\nfor handling big data.\nWhile they are fine languages to work with, I want to throw Node.js\ninto the mix as a serious contender to get the job done.\nNode.js is performant enough to do some heavy lifting and has great NPM packages\nthat help you develop fast. I wanted to share my experience of building data-intensive systems using Node.js.</p>\n<p>I've been working at <a href=\"https://www.coupang.com/\">Coupang</a> for just over a year now. I had the privilege of building the first\n<a href=\"https://en.wikipedia.org/wiki/Extract,_transform,_load\">ETL</a>\nsystem for Coupang's advertising platform.\nThe ETL system began with a modest goal to clean and ingest advertising\nmetadata and time series data on a daily basis.\nSince the original ETL jobs began every day in the morning, I named\nthe ETL system, <code>Morning Star</code>, referring to planet Venus that appears in\nthe east before sunrise.\nThis is a bit of a misnomer now, given that the system is now running\nETL jobs on a real-time basis. I still kept the name.\nSince its inception, Morning Star evolved into a much larger system\nthat is handling 500 GB of data, 400 million events per day, and the data volume is still growing.</p>\n<p>Morning Star needed to provide data to several kinds of users. Services\nneeded to retrieve appropriate data reliably in a timely fashion.\nAnalysts asked for a place where they can gather insights about our business. Also, Morning Star needed to provide reliable, normalized data for creating denormalized tables for specific use cases. </p>\n<p>Let's examine the basic data flow of Morning Star. Morning Star's\ninfrastructure lives on AWS.\nThe data flow starts with several Lambda functions that trigger\ndifferent jobs for Morning Star by making an HTTP request to the\nload balancer.\nThe load balancer balances the load in a round robin fashion to\nEC2 instances. Based on the CPU usages, the EC2 instances dynamically\nscale up or down to efficiently handle the load.\nThe EC2 instance that is given the request to handle a job, downloads\nthe raw files from S3 Data Lake. The raw files in S3 originate\nfrom several other\nservices. Morning Star cleans, enriches and anonymizes the raw data depending on which job it has to perform.\nThe massaged data is then uploaded back to the data lake, where\nit is subsequently copied to Redshift.\nRedshift is where various other services and users can consume the\ndata to their liking.</p>\n<p><img src=\"https://i.imgur.com/BsZ1i9t.png\"></p>\n<p>Node.js has been an excellent tool for building the desired system.\nNode.js finally has the <code>async-await</code> pattern to take advantage of\nparallelism without callbacks.\nHaving worked with callbacks and chained promises, I think\nthe async-await pattern solved major problems found in the previous\nasynchronous programming patterns.\nYou can program your code spanning vertically rather than horizontally\nlike the nested callbacks make you do. Your code is easier to read\nwhen it spans vertically.\nYou can handle errors more precisely than the chained promises pattern where\na single catch block is typically responsible for errors\nthrown anywhere during the execution of multiple chained\npromises.\nYou can have a more nimble control flow of your code with <code>async-await</code>\nbecause you can use <code>if-else</code> statements easier.\nThe ergonomics of coding was much improved with using async-await.</p>\n<p>Speaking of great ergonomics, I should also point out the <code>stream</code>\nmodule in Node.js to be a fantastic tool to process large files.\nWhen you work on web services, you typically read and write\ndiscrete chunks of data transactionally.\nOn the other hand, it is very common to use stream to move\ndata from one place another in data engineering.\nYou often deal with cases where you no longer can load\nthe entire data onto the memory.\nOr you might deal with a source that is continuously generating data that\nneeds to be moved to another place in real time.\nThat's when you have to stream the data.\nNode's stream library helps you stream data reliably and manipulate\nthe data with ease.\nThe functions in the stream module are easy to understand and use.\nThey are well documented with an ample amount of examples, so you can easily\nincorporate them into your code.\nThe stream library plays nicely with the async-await pattern I use\nelsewhere as long as I wrap the stream code with a promise.</p>\n<p>There are other standard modules in Node.js that makes it\nsuitable to do data engineering.\nThe <code>child-process</code> module is used to execute terminal commands,\nand it was useful in controlling\n<a href=\"https://stedolan.github.io/jq/\">jq</a>,\n<a href=\"https://www.gnu.org/software/parallel/\">parallel</a> and\n<a href=\"https://cwiki.apache.org/confluence/display/Hive/HiveServer2+Clients#HiveServer2Clients-Beeline%E2%80%93CommandLineShell\">Hive Beeline client</a>.\nThe <code>fs</code> module had all the functions I needed to work with the file system,\nand <code>zlib</code> handle gzipping and gunzipping with stream perfectly.\nAnd if I didn't find an adequate module in the standard Node.js modules,\nI found almost all of them in the NPM repository.\nThis assurance gave me the confidence to push development in Node.js,\nand help me iterate faster.</p>\n<p>Node.js has some downsides when you try to use it to process data.\nI applauded Node.js' async-await to be an excellent pattern to use.\nHowever, this means that you have to write a promise wrapper for\ncallbacks and pipe functions in order to incorporate it with the\nasync-await pattern. It's not difficult, but it is tedious.</p>\n<p>I also had trouble finding an up to date Hive client for Node.js.\nHive is a widely used data warehousing tool for working with big data.\nI was surprised to learn that there's no well maintained\nHive client available to use on NPM.\nFor this reason, I had to use Hive's Beeline command line client and\nwrite Node wrapper around it.\nI had to install JDK, Hadoop, and Hive as dependencies for Beeline\nto run.\nThere were a lot of unpleasant programming I had to do to make this\nwork.</p>\n<p>Despite some of the problems with Node.js, it is still a great tool\nfor data engineering. It has many standard and third-party modules\nto help to finish work. Node.js is also fast enough to process large datasets\nin real-time.\nI knew that Node.js is great at creating nice web apps and services.\nI am glad to know that it is also a fine tool for data engineering.</p>\n"}},"__N_SSG":true}